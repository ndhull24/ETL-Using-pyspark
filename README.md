# ETL-Using-pyspark

An ETL (Extract, Transform, and Load) pipeline extracts data from various sources, transforms it into a structured format, and loads it into a storage system for further analysis. PySpark is an excellent choice for building ETL pipelines due to its distributed computing capabilities, high performance, and ability to handle both structured and unstructured data efficiently. This project demonstrates how to build a scalable ETL pipeline using PySpark, focusing on processing temperature-related data from multiple countries spanning from 1961 to 2022.
